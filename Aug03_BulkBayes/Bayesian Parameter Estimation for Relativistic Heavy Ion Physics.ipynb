{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JETSCAPE Summer School 2022: \n",
    "## Bayesian parameter inference for a **relativistic heavy ion collision model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from scipy import optimize\n",
    "from scipy.linalg import lapack\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process import kernels as krnl\n",
    "\n",
    "\n",
    "import emcee # For MCMC\n",
    "import ptemcee # For Parallel tempered MCMC\n",
    "#from emcee import PTSampler\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import time\n",
    "sns.set(\"notebook\")\n",
    "\n",
    "# check if you can load the pregenerated emulators:\n",
    "with open(\"./DataFiles/PbPb2760_emulators_scikit.dat\",\"rb\") as f:\n",
    "    _ = pickle.load(f)\n",
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "colors = OrderedDict([\n",
    "    ('blue', '#4e79a7'),\n",
    "    ('orange', '#f28e2b'),\n",
    "    ('green', '#59a14f'),\n",
    "    ('red', '#e15759'),\n",
    "    ('cyan', '#76b7b2'),\n",
    "    ('purple', '#b07aa1'),\n",
    "    ('brown', '#9c755f'),\n",
    "    ('yellow', '#edc948'),\n",
    "    ('pink', '#ff9da7'),\n",
    "    ('gray', '#bab0ac')\n",
    "])\n",
    "\n",
    "fontsize = dict(\n",
    "    large=11,\n",
    "    normal=10,\n",
    "    small=9,\n",
    "    tiny=8\n",
    ")\n",
    "\n",
    "offblack = '.15'\n",
    "\n",
    "plt.rcdefaults()\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Lato'],\n",
    "    'mathtext.fontset': 'custom',\n",
    "    'mathtext.default': 'it',\n",
    "    'mathtext.rm': 'sans',\n",
    "    'mathtext.it': 'sans:italic:medium',\n",
    "    'mathtext.cal': 'sans',\n",
    "    'font.size': fontsize['normal'],\n",
    "    'legend.fontsize': fontsize['normal'],\n",
    "    'axes.labelsize': fontsize['normal'],\n",
    "    'axes.titlesize': fontsize['large'],\n",
    "    'xtick.labelsize': fontsize['small'],\n",
    "    'ytick.labelsize': fontsize['small'],\n",
    "    #'font.weight': 400,\n",
    "    'axes.labelweight': 400,\n",
    "    'axes.titleweight': 400,\n",
    "    'axes.prop_cycle': plt.cycler('color', list(colors.values())),\n",
    "    'lines.linewidth': .8,\n",
    "    'lines.markersize': 3,\n",
    "    'lines.markeredgewidth': 0,\n",
    "    'patch.linewidth': .8,\n",
    "    'axes.linewidth': .6,\n",
    "    'xtick.major.width': .6,\n",
    "    'ytick.major.width': .6,\n",
    "    'xtick.minor.width': .4,\n",
    "    'ytick.minor.width': .4,\n",
    "    'xtick.major.size': 3.,\n",
    "    'ytick.major.size': 3.,\n",
    "    'xtick.minor.size': 2.,\n",
    "    'ytick.minor.size': 2.,\n",
    "    'xtick.major.pad': 3.5,\n",
    "    'ytick.major.pad': 3.5,\n",
    "    'axes.labelpad': 4.,\n",
    "    'axes.formatter.limits': (-5, 5),\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'text.color': offblack,\n",
    "    'axes.edgecolor': offblack,\n",
    "    'axes.labelcolor': offblack,\n",
    "    'xtick.color': offblack,\n",
    "    'ytick.color': offblack,\n",
    "    'legend.frameon': False,\n",
    "    'image.cmap': 'Blues',\n",
    "    'image.interpolation': 'none',\n",
    "})\n",
    "\n",
    "def set_tight(fig=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Set tight_layout with a better default pad.\n",
    "\n",
    "    \"\"\"\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    kwargs.setdefault('pad', .1)\n",
    "    fig.set_tight_layout(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"JETSCAPE_bayes\"\n",
    "#Saved emulator name\n",
    "EMU='PbPb2760_emulators_scikit.dat'\n",
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results\"\n",
    "FIGURE_ID = \"Results/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\"\n",
    "\n",
    "# Define folder structure \n",
    "\n",
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a Bayesian Parameter Inference similar to that in the published paper by **Derek Everett** and the JETSCAPE collaborators (2021):\n",
    "- [Multi-system Bayesian constraints on the transport coefficients of QCD matter](https://arxiv.org/abs/2011.01430), [Phys. Rev. C 103, 054904 (2021)](https://journals.aps.org/prc/abstract/10.1103/PhysRevC.103.054904).\n",
    "- [Phenomenological constraints on the transport properties of QCD matter with data-driven model averaging](https://arxiv.org/abs/2010.03928), [Phys. Rev. Lett. 126, 242301 (2021)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.242301).\n",
    "\n",
    "In the paper they consider multiple choices of particularization models and used both RHIC and LHC data. But in this hands-on session, to keep things simple, we will limit to one model and only the LHC experimental data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JETSCAPE framework for the evolution of the medium in high-energy nuclear collisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Different stages of relativistic heavy ion collision](IllustrationFigs/JETSCAPE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. The Piror**\n",
    "   - Understand the meaning of the parameters and the prior range of varaiation.\n",
    "   - What happens if we restricted a few prior parameters (in the end).\n",
    "\n",
    "**1. Simulation data**\n",
    "   - Load saved simulation data from a folder. Anyone who wish to use the code to do another Bayesian analysis for relativistic heavy ion collision should do minor change to this section that suits their model and simulation data.\n",
    "            --Python modules used\n",
    "               - pandas\n",
    "    \n",
    "**2. Data preprocessing**\n",
    "   - Standardize observables by removing the mean and scaling to unit variance. \n",
    "   - Dimensionality reduction of the data using the Principal Component Analysis (PCA).\n",
    "             --Python modules used\n",
    "                - numpy, sklearn\n",
    "                \n",
    "**3. Experimental data**\n",
    "   - Relevant experimental data is also loaded from disk.\n",
    "   - Compare prior samples of observables to the experimental data.\n",
    "            --Python modules used\n",
    "                - pandas\n",
    "\n",
    "**4. Building Emulators**\n",
    "   - Since the simulations are computationally expensive, we need surrogate models that can be trained on a sparse set of simulation data obtained from the full model.\n",
    "   - Train Gaussian Process Emulators (GP) for each principal component (truncated). Construct the surrogate model \"GP+inverse PCA\" to fast interpolate model calculations with emulator uncertainties.\n",
    "   - Validate the emulators.\n",
    "            --Python modules used\n",
    "                - sklearn Gaussian Processors, numpy\n",
    "                \n",
    "**5. Bayesian Parameter Estimation**\n",
    "   - The Bayes theorem is used to define the posterior of the model parameters using the emulators and the experimental data.\n",
    "   - Random samples are drawn from the high-dimensional posterior using Markov Chain Monte Carlo (MCMC) method.\n",
    "   - Study the posterior viscosity and observables.\n",
    "            --Python modules used\n",
    "                - pandas, numpy, seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Understanding the Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JETSCAPE model parameters and prior range of individual parameters\n",
    "\n",
    "![Model parameters](IllustrationFigs/JS-prior.png)\n",
    "\n",
    "Eight of these parameters are related to the temperature-dependent shear and bulk viscosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def zeta_over_s(T, zmax, T0, width, asym):\n",
    "    DeltaT = T - T0\n",
    "    sign = 1 if DeltaT>0 else -1\n",
    "    x = DeltaT/(width*(1.+asym*sign))\n",
    "    return zmax/(1.+x**2)\n",
    "\n",
    "@np.vectorize\n",
    "def eta_over_s(T, T_k, alow, ahigh, etas_k):\n",
    "    if T < T_k:\n",
    "        y = etas_k + alow*(T-T_k)\n",
    "    else:\n",
    "        y = etas_k + ahigh*(T-T_k)\n",
    "    if y > 0:\n",
    "        return y\n",
    "    else:\n",
    "        return 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "What the prior of $\\eta/s(T)$ and $\\zeta/s(T)$ look like, assuming $T_\\eta, (\\eta/s)_{\\rm kink}, a_{\\rm low}, a_{\\rm high}$ and $(\\zeta/s)_{\\rm max}, T_\\zeta, w_\\zeta, \\lambda_\\eta$ are pulled from independent uniform distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4,3.5),\n",
    "                         sharex=False, sharey=False, constrained_layout=True)\n",
    "fig.suptitle(\"Specefic shear viscosity posterior\", wrap=True)\n",
    "ax.set_xlabel(r'$T [GeV]$')\n",
    "ax.set_ylabel(r'$\\eta/s(T)$')\n",
    "\n",
    "u4 = np.random.rand(4, 1000)\n",
    "Tk    =  0.13 * (1.-u4[0]) + 0.3 * u4[0]\n",
    "alow  = - 2.0 * (1.-u4[1]) + 1.0 * u4[1] \n",
    "ahigh = - 1.0 * (1.-u4[2]) + 2.0 * u4[2] \n",
    "etask =  0.01 * (1.-u4[3]) + 0.2 * u4[3] \n",
    "\n",
    "T = np.linspace(0.1, 0.5, 100)\n",
    "ax.set_xlim(0.1, 0.6)\n",
    "\n",
    "etas_samples = np.array([eta_over_s(it, Tk, alow, ahigh, etask) for it in T]).T\n",
    "ax.set_ylim(0,np.max(etas_samples))\n",
    "\n",
    "prior_median = np.percentile(etas_samples, 50, axis=0)\n",
    "ax.plot(T, prior_median, 'k-')\n",
    "\n",
    "_ = prior_median.max()\n",
    "ax.annotate('Prior median', xy=(T[-1]*1.02, _), va=\"bottom\", ha=\"left\")\n",
    "for CL in [60,90,97.5,100]:\n",
    "    Low, High = np.percentile(etas_samples, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(T, Low, High, alpha=0.1, color='k')\n",
    "    _ = (_ + High.max())/2.\n",
    "    ax.annotate(\"{}%CL\".format(CL), xy=(T[-1]*1.02, _), va=\"center\", ha=\"left\")\n",
    "    _ = High.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please repeat for bulk viscosity\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4,3.5),\n",
    "                         sharex=False, sharey=False, constrained_layout=True)\n",
    "fig.suptitle(\"Bulk shear viscosity posterior\", wrap=True)\n",
    "ax.set_xlabel(r'$T [GeV]$')\n",
    "ax.set_ylabel(r'$\\zeta/s(T)$')\n",
    "\n",
    "u4 = np.random.rand(4, 10000)\n",
    "zetasmax =  0.01 * (1.-u4[0]) + 0.2 * u4[0]\n",
    "Tzeta  = 0.12 * (1.-u4[1]) + 0.3 * u4[1] \n",
    "wzeta = 0.025 * (1.-u4[2]) + 0.15 * u4[2] \n",
    "lambdazeta =  -0.8 * (1.-u4[3]) + 0.8 * u4[3] \n",
    "\n",
    "T = np.linspace(0.1, 0.5, 100)\n",
    "ax.set_xlim(0.1, 0.6)\n",
    "\n",
    "zetas_samples = np.array([zeta_over_s(it, zetasmax, Tzeta, wzeta, lambdazeta) for it in T]).T\n",
    "ax.set_ylim(0,np.max(zetas_samples))\n",
    "\n",
    "prior_median = np.percentile(zetas_samples, 50, axis=0)\n",
    "ax.plot(T, prior_median, 'k-')\n",
    "\n",
    "_ = prior_median.max()\n",
    "ax.annotate('Prior median', xy=(.2, _), va=\"bottom\", ha=\"center\")\n",
    "for CL in [60,90,97.5,100]:\n",
    "    Low, High = np.percentile(zetas_samples, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(T, Low, High, alpha=0.1, color='k')\n",
    "    _ = (_ + High.max())/2.\n",
    "    ax.annotate(\"{}%CL\".format(CL), xy=(.2, _), va=\"center\", ha=\"center\")\n",
    "    _ = High.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any long-range correlations in the Prior?\n",
    "For example, if we somehow use RHIC data to constrain $\\eta/s$ to be within $0.15<\\eta/s<0.2$ below $T=0.2$, does the prior become too restrictive at higher temperature that it may cause tension when new (e.g. LHC) data that are senstivie to higher temperature (e.g. 0.4 GeV) region are included?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = T<0.2\n",
    "selected_samples = []\n",
    "Thigh_samples = []\n",
    "while len(Thigh_samples) < 2000:\n",
    "    u = np.random.rand(4)\n",
    "    Tk    =  0.13 * (1.-u[0]) + 0.3 * u[0]\n",
    "    alow  = - 2.0 * (1.-u[1]) + 1.0 * u[1] \n",
    "    ahigh = - 1.0 * (1.-u[2]) + 2.0 * u[2] \n",
    "    etask =  0.01 * (1.-u[3]) + 0.2 * u[3] \n",
    "    it = eta_over_s(T, Tk, alow, ahigh, etask)\n",
    "    if ( (0.15<it[cut]) & (it[cut]<0.2) ).all():\n",
    "        Thigh_samples.append(eta_over_s([0.4], Tk, alow, ahigh, etask)[0])\n",
    "        if np.random.rand()<.1:\n",
    "            \n",
    "            selected_samples.append(it)\n",
    "selected_samples = np.array(selected_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axL, axR) = plt.subplots(1,2, figsize=(8,3.5))\n",
    "\n",
    "axL.set_title(\"selected samples w. lower-$T$ constraints\")\n",
    "axL.set_xlabel(r\"$T [GeV]$\")\n",
    "axL.set_ylabel(r\"$\\eta/s$\")\n",
    "axL.plot(T, selected_samples.T, color='r', alpha=0.2)\n",
    "        \n",
    "axR.set_title(\"Prior at high-$T$ before/after low-$T$ constraints\")\n",
    "axR.hist(etas_samples[:,-1], bins=15, color='k', histtype='step', density=True)\n",
    "axR.hist(Thigh_samples, bins=15, color='r', histtype='step', density=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training data: designed inputs and model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Model parameter names in Latex compatble form\n",
    "model_param_dsgn = ['$N$[$2.76$TeV]',\n",
    " '$p$',\n",
    " '$\\\\sigma_k$',\n",
    " '$w$ [fm]',\n",
    " '$d_{\\\\mathrm{min}}$ [fm]',\n",
    " '$\\\\tau_R$ [fm/$c$]',\n",
    " '$\\\\alpha$',\n",
    " '$T_{\\\\eta,\\\\mathrm{kink}}$ [GeV]',\n",
    " '$a_{\\\\eta,\\\\mathrm{low}}$ [GeV${}^{-1}$]',\n",
    " '$a_{\\\\eta,\\\\mathrm{high}}$ [GeV${}^{-1}$]',\n",
    " '$(\\\\eta/s)_{\\\\mathrm{kink}}$',\n",
    " '$(\\\\zeta/s)_{\\\\max}$',\n",
    " '$T_{\\\\zeta,c}$ [GeV]',\n",
    " '$w_{\\\\zeta}$ [GeV]',\n",
    " '$\\\\lambda_{\\\\zeta}$',\n",
    " '$b_{\\\\pi}$',\n",
    " '$T_{\\\\mathrm{sw}}$ [GeV]']\n",
    "\n",
    "# Design points\n",
    "design = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_design\")\n",
    "print(\"number of design & dimension of design\", design.shape)\n",
    "design.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Experimental observables\n",
    "\n",
    "   - Charged particle multiplicity $\\frac{dN_{\\rm ch}}{d\\eta}$.\n",
    "   - Transverse energy $\\frac{dE_{\\rm T}}{d\\eta}$.\n",
    "   - Identified particle yield and mean tranverse momentum $\\frac{dN_{\\pi, K, p}}{dy}, \\langle p_{T}\\rangle_{\\pi, K, p}$.\n",
    "   - Flow harmonics $\\frac{dN}{d\\phi} \\propto 1 + \\sum_n 2v_n \\cos(n-\\Psi_n)$.\n",
    "   \n",
    "       The observables are $v_n\\{2\\} = \\sqrt{\\langle v_n^2 \\rangle}$: $v_2\\{2\\}, v_2\\{3\\}, v_2\\{4\\}$\n",
    "       \n",
    "   - Charged-particle mean-$p_T$ event-by-event flucutations\n",
    "       $\\frac{\\delta\\langle p_T\\rangle}{\\langle p_T\\rangle}$\n",
    "\n",
    "![Different stages of relativistic heavy ion collision](IllustrationFigs/inversion-problem-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the experiment observables (110,) and shape of the experimental error variance(110,)\n"
     ]
    }
   ],
   "source": [
    "experiment=pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_experiment\",index_col=0)\n",
    "experiment.head()\n",
    "y_exp=experiment.loc['mean'].values\n",
    "y_exp_variance=experiment.loc['variance'].values\n",
    "print(f'Shape of the experiment observables {y_exp.shape} and shape of the experimental error variance{y_exp_variance.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 bins\n",
    "ALICE_cent_bins = np.array([[0,5],[5,10],[10,20],[20,30],[30,40],[40,50],[50,60],[60,70]]) \n",
    "\n",
    "obs_cent_list = {\n",
    "'Pb-Pb-2760': {\n",
    "    'dNch_deta' : ALICE_cent_bins,\n",
    "    'dET_deta' : np.array([[0, 2.5], [2.5, 5], [5, 7.5], [7.5, 10],\n",
    "                           [10, 12.5], [12.5, 15], [15, 17.5], [17.5, 20],\n",
    "                           [20, 22.5], [22.5, 25], [25, 27.5], [27.5, 30],\n",
    "                           [30, 32.5], [32.5, 35], [35, 37.5], [37.5, 40],\n",
    "                           [40, 45], [45, 50], [50, 55], [55, 60],\n",
    "                           [60, 65], [65, 70]]), # 22 bins\n",
    "    'dN_dy_pion'   : ALICE_cent_bins,\n",
    "    'dN_dy_kaon'   : ALICE_cent_bins,\n",
    "    'dN_dy_proton' : ALICE_cent_bins,\n",
    "    'dN_dy_Lambda' : np.array([[0,5],[5,10],[10,20],[20,40],[40,60]]), # 5 bins\n",
    "    'dN_dy_Omega'  : np.array([[0,10],[10,20],[20,40],[40,60]]), # 4 bins\n",
    "    'dN_dy_Xi'     : np.array([[0,10],[10,20],[20,40],[40,60]]), # 4 bins\n",
    "    'mean_pT_pion'   : ALICE_cent_bins,\n",
    "    'mean_pT_kaon'   : ALICE_cent_bins,\n",
    "    'mean_pT_proton' : ALICE_cent_bins,\n",
    "    'pT_fluct' : np.array([[0,5],[5,10],[10,15],[15,20], [20,25],[25,30],[30,35],[35,40], [40,45],[45,50],[50,55],[55,60]]), #12 bins\n",
    "    'v22' : ALICE_cent_bins,\n",
    "    'v32' : np.array([[0,5],[5,10],[10,20],[20,30], [30,40],[40,50]]), # 6 bins\n",
    "    'v42' : np.array([[0,5],[5,10],[10,20],[20,30], [30,40],[40,50]]), # 6 bins\n",
    "    }\n",
    "}\n",
    "\n",
    "obs_groups = {'yields' : ['dNch_deta', 'dET_deta', 'dN_dy_pion', 'dN_dy_kaon', 'dN_dy_proton'],\n",
    "              'mean_pT' : ['mean_pT_pion', 'mean_pT_kaon','mean_pT_proton', ],\n",
    "              'fluct' : ['pT_fluct'],\n",
    "              'flows' : ['v22', 'v32', 'v42']}\n",
    "\n",
    "obs_group_labels = {'yields' : r'$dN_\\mathrm{id}/dy_p$, $dN_\\mathrm{ch}/d\\eta$, $dE_T/d\\eta$ [GeV]',\n",
    "                    'mean_pT' : r'$ \\langle p_T \\rangle_\\mathrm{id}$' + ' [GeV]',\n",
    "                    'fluct' : r'$\\delta p_{T,\\mathrm{ch}} / \\langle p_T \\rangle_\\mathrm{ch}$',\n",
    "                    'flows' : r'$v^{(\\mathrm{ch})}_k\\{2\\} $'}\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'tan', 'gray']\n",
    "\n",
    "obs_tex_labels = {'dNch_deta' : r'$dN_\\mathrm{ch}/d\\eta$' + ' x 2',\n",
    "                  'dN_dy_pion' : r'$dN_{\\pi}/dy_p$',\n",
    "                  'dN_dy_kaon' : r'$dN_{K}/dy_p$',\n",
    "                  'dN_dy_proton' : r'$dN_{p}/dy_p$',\n",
    "                  'dET_deta' : r'$dE_{T}/d\\eta$' + ' x 5',\n",
    "                  \n",
    "                  'mean_pT_proton' : r'$\\langle p_T \\rangle_p$',\n",
    "                  'mean_pT_kaon' : r'$\\langle p_T \\rangle_K$',\n",
    "                  'mean_pT_pion' : r'$\\langle p_T \\rangle_\\pi$',\n",
    "                 \n",
    "                  'pT_fluct' : None,\n",
    "                  'v22' : r'$v^{(\\mathrm{ch})}_2\\{2\\}$',\n",
    "                  'v32' : r'$v^{(\\mathrm{ch})}_3\\{2\\}$',\n",
    "                  'v42' : r'$v^{(\\mathrm{ch})}_4\\{2\\}$'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index={}\n",
    "st_index=0\n",
    "for obs_group in  obs_groups.keys():\n",
    "    for obs in obs_groups[obs_group]:\n",
    "        #print(obs)\n",
    "        n_centrality= len(obs_cent_list['Pb-Pb-2760'][obs])\n",
    "        #print(n_centrality)\n",
    "        index[obs]=[st_index,st_index+n_centrality]\n",
    "        st_index = st_index+n_centrality\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 5), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for row, obs_group in enumerate(obs_groups.keys()):\n",
    "    for obs, color in zip(obs_groups[obs_group], colors):\n",
    "        expt_label = 'ALICE'\n",
    "        \n",
    "        axes[row].tick_params(labelsize=9)\n",
    "\n",
    "        scale = 1.0\n",
    "        if obs_group == 'yields':\n",
    "            axes[row].set_yscale('log')         \n",
    "            axes[row].set_title(\"Experimental Data\", fontsize = 11)\n",
    "            if obs == 'dET_deta':\n",
    "                scale = 5.\n",
    "            if obs == 'dNch_deta':\n",
    "                scale = 2.\n",
    "        \n",
    "        axes[row].set_ylabel(obs_group_labels[obs_group], fontsize = 11)\n",
    "        \n",
    "        xbins = np.array(obs_cent_list['Pb-Pb-2760'][obs])\n",
    "        x = (xbins[:,0] + xbins[:,1]) / 2.\n",
    "\n",
    "        label = obs_tex_labels[obs]\n",
    "            \n",
    "    \n",
    "        exp_mean = y_exp[index[obs][0]:index[obs][1]]\n",
    "        exp_err = np.sqrt(y_exp_variance[index[obs][0]:index[obs][1]])\n",
    "\n",
    "        axes[row].errorbar(x, exp_mean*scale, exp_err, color=color, fmt='v', markersize='4', elinewidth=1, label=label)\n",
    " \n",
    "        \n",
    "    leg = axes[row].legend(fontsize=9, borderpad=0, labelspacing=0, handlelength=1, handletextpad=0.2)\n",
    "    \n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "        legobj.set_alpha(1.0)\n",
    "\n",
    "    axes[row].set_xlim(0, 70)\n",
    "    if obs_group == 'yields':\n",
    "        axes[row].set_ylim(1, 1e4)\n",
    "    if obs_group == 'mean_pT':\n",
    "        axes[row].set_ylim(0., 1.5)\n",
    "    if obs_group == 'fluct':\n",
    "        axes[row].set_ylim(0.0, 0.04)\n",
    "    if obs_group == 'flows':\n",
    "        axes[row].set_ylim(0.0, 0.12)\n",
    "    if axes[row].is_last_row():\n",
    "        axes[row].set_xlabel('Centrality %', fontsize = 11)\n",
    "\n",
    "plt.tight_layout(True)\n",
    "set_tight(fig, rect=[0, 0, 1, 1])\n",
    "save_fig(\"Experimental_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training data\n",
    "\n",
    "Observables calculated from the physical model simulations at each of the design parameter set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation outputs at the design points\n",
    "simulation = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_simulation\")\n",
    "print(\"Number of physical evaluations, dimension of observable vector\", simulation.shape)\n",
    "simulation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation outputs at the design points\n",
    "validation = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_simulation_validation\")\n",
    "print(\"Number of physical evaluations, dimension of observable vector\", validation.shape)\n",
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design points\n",
    "design_validation = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_2760_validation_design\")\n",
    "print(\"number of design & dimension of design\", design_validation.shape)\n",
    "design_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 5), sharex=True)\n",
    "axes =  axes.flatten()\n",
    "for row, obs_group in enumerate(obs_groups.keys()):\n",
    "    for obs, color in zip(obs_groups[obs_group], colors):\n",
    "        expt_label = 'ALICE'\n",
    "        \n",
    "        axes[row].tick_params(labelsize=9)\n",
    "\n",
    "        scale = 1.0\n",
    "        if obs_group == 'yields':\n",
    "            axes[row].set_yscale('log')\n",
    "            axes[row].set_title(\"Design vs Data\", fontsize = 11)          \n",
    "            if obs == 'dET_deta':\n",
    "                scale = 5.\n",
    "            if obs == 'dNch_deta':\n",
    "                scale = 2.\n",
    "        \n",
    "        axes[row].set_ylabel(obs_group_labels[obs_group], fontsize = 11)\n",
    "        \n",
    "        xbins = np.array(obs_cent_list['Pb-Pb-2760'][obs])\n",
    "        x = (xbins[:,0] + xbins[:,1]) / 2.\n",
    "\n",
    "        for i in range(simulation.shape[0]):\n",
    "            Y1 = simulation.iloc[i,index[obs][0]:index[obs][1]]\n",
    "            axes[row].plot(x, Y1*scale, color=color, lw=1, alpha=.1)\n",
    "        label = obs_tex_labels[obs]           \n",
    "       \n",
    "\n",
    "        exp_mean = y_exp[index[obs][0]:index[obs][1]]\n",
    "        exp_err = np.sqrt(y_exp_variance[index[obs][0]:index[obs][1]])\n",
    "\n",
    "        axes[row].errorbar(x, exp_mean*scale, exp_err, color='black', fmt='v', markersize='4', elinewidth=1)\n",
    "        \n",
    "    leg = axes[row].legend(fontsize=9, borderpad=0, labelspacing=0, handlelength=1, handletextpad=0.2)\n",
    "    \n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "        legobj.set_alpha(1.0)\n",
    "\n",
    "    axes[row].set_xlim(0, 70)\n",
    "\n",
    "    if obs_group == 'yields':\n",
    "        axes[row].set_ylim(1, 1e5)\n",
    "    if obs_group == 'mean_pT':\n",
    "        axes[row].set_ylim(0., 2)\n",
    "    if obs_group == 'fluct':\n",
    "        axes[row].set_ylim(0.0, 0.08)\n",
    "    if obs_group == 'flows':\n",
    "        axes[row].set_ylim(0.0, 0.16)\n",
    "    if axes[row].is_last_row():\n",
    "        axes[row].set_xlabel('Centrality %', fontsize = 11)\n",
    "\n",
    "plt.tight_layout(True)\n",
    "set_tight(fig, rect=[0, 0, 1, 1])\n",
    "save_fig(\"Design-vs-Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a necessary (but not always sufficient) condiiton, one should alwasy makesure that the traning data provide a good coverage of the experimental data. \n",
    "   - If not, then the model emulator will not have a good predictive power (good control of uncertainty) to analyze the model's behavior in the parameter regions that describes the data.\n",
    "   - However, even if there is a good \"coverage\", it is possible that due to correlation among different obserable, there are still tensions in simultaneously descrbining all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Statistical Analysis Begins Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Training data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the tranining data: design inputs and observables\n",
    "X = design.values\n",
    "Y = simulation.values\n",
    "print( \"Dims of design (X)\"+ str(X.shape) )\n",
    "print( \"Dims of output observables (Y) at these design: \"+ str(Y.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 485 sets of 110 observables, with different obervables have drastically magnitude.\n",
    "For example, $\\frac{dN_{\\rm ch}}{d\\eta} = 10^{3\\sim 4}, v_2 = 0.01\\sim0.1$. We will standardlize each observable by removing the mean and devide the standard deviation:\n",
    "\n",
    "$${\\rm O'} = \\frac{O - \\bar{\\rm O}}{\\sqrt{\\bar{\\rm O^2}-\\bar{O}^2}}$$\n",
    "\n",
    "where the \"bar\" stands for averging over the use of different sets of training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data to be zero mean and unit variance for each observables\n",
    "SS  =  StandardScaler(copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Dimensional Reduction of the Observables via the Principal Component Analysis (PCA)\n",
    "\n",
    "Discussion:\n",
    "   - Is it alwasy necessary?\n",
    "   - What's the different between \n",
    "      - 1) Directly train emulators on each individual observables;\n",
    "      - 2) Use PCA, and training emulators on each PC;\n",
    "      - 3) Use PCA, and training emulators on a truncated set of PC?\n",
    "   - How many Principal Components should we keep (truncation)?\n",
    "      - 1) Accuracy, how to estimate the performance of the truncated emualtor?\n",
    "      - 2) Efficiency. How does the time complexity rest of the analysis scales with $N_{\\rm pc}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Singular Value Decomposition\n",
    "u, s, vh = np.linalg.svd(SS.fit_transform(Y), full_matrices=True)\n",
    "print(f'shape of u {u.shape} shape of s {s.shape} shape of vh {vh.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the explained raito of variance\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,3))\n",
    "#importance = pca_analysis.explained_variance_\n",
    "importance = np.square(s[:40]/math.sqrt(u.shape[0]-1))\n",
    "cumulateive_importance = np.cumsum(importance)/np.sum(importance)\n",
    "idx = np.arange(1,1+len(importance))\n",
    "ax1.bar(idx,importance)\n",
    "ax1.set_xlabel(\"PC index\")\n",
    "ax1.set_ylabel(\"Variance\")\n",
    "ax1.semilogy()\n",
    "ax2.bar(idx,cumulateive_importance)\n",
    "ax2.set_xlabel(r\"The first $n$ PC\")\n",
    "ax2.set_ylabel(\"Fraction of total variance\")\n",
    "plt.tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "   - I have put the importance/variance of the PCs in log scale. Can you understand the \"kink\" feature around index=13? What caused this transition?\n",
    "   - What does this suggests for you maximum possible number of PCs to be included in your emulator traning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whiten and project data to principal component axis (only keeping first 10 PCs)\n",
    "Npc = 15\n",
    "pc_tf_data=u[:,:Npc] * math.sqrt(u.shape[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load priors and experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for parametrs in the emulator are same as prior ranges so\n",
    "prior_df = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_prior\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>norm</th>\n",
       "      <th>trento_p</th>\n",
       "      <th>sigma_k</th>\n",
       "      <th>nucleon_width</th>\n",
       "      <th>dmin3</th>\n",
       "      <th>tau_R</th>\n",
       "      <th>alpha</th>\n",
       "      <th>eta_over_s_T_kink_in_GeV</th>\n",
       "      <th>eta_over_s_low_T_slope_in_GeV</th>\n",
       "      <th>eta_over_s_high_T_slope_in_GeV</th>\n",
       "      <th>eta_over_s_at_kink</th>\n",
       "      <th>zeta_over_s_max</th>\n",
       "      <th>zeta_over_s_T_peak_in_GeV</th>\n",
       "      <th>zeta_over_s_width_in_GeV</th>\n",
       "      <th>zeta_over_s_lambda_asymm</th>\n",
       "      <th>shear_relax_time_factor</th>\n",
       "      <th>Tswitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.913</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     norm  trento_p  sigma_k  nucleon_width  dmin3  tau_R  alpha  \\\n",
       "min  10.0      -0.7      0.3            0.5  0.000    0.3   -0.3   \n",
       "max  20.0       0.7      2.0            1.5  4.913    2.0    0.3   \n",
       "\n",
       "     eta_over_s_T_kink_in_GeV  eta_over_s_low_T_slope_in_GeV  \\\n",
       "min                      0.13                           -2.0   \n",
       "max                      0.30                            1.0   \n",
       "\n",
       "     eta_over_s_high_T_slope_in_GeV  eta_over_s_at_kink  zeta_over_s_max  \\\n",
       "min                            -1.0                0.01             0.01   \n",
       "max                             2.0                0.20             0.20   \n",
       "\n",
       "     zeta_over_s_T_peak_in_GeV  zeta_over_s_width_in_GeV  \\\n",
       "min                       0.12                     0.025   \n",
       "max                       0.30                     0.150   \n",
       "\n",
       "     zeta_over_s_lambda_asymm  shear_relax_time_factor  Tswitch  \n",
       "min                      -0.8                      2.0    0.130  \n",
       "max                       0.8                      8.0    0.165  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_max=prior_df.loc['max'].values\n",
    "design_min=prior_df.loc['min'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "design=X\n",
    "overide=False\n",
    "input_dim=len(design_max)\n",
    "ptp = design_max - design_min\n",
    "bound=zip(design_min,design_max)\n",
    "\n",
    "if (os.path.exists(data_path(EMU))) and (overide==False):\n",
    "    print('Saved emulators exists and overide is prohibited')\n",
    "    with open(data_path(EMU),\"rb\") as f:\n",
    "        Emulators = pickle.load(f)\n",
    "        for i, GPR in enumerate(Emulators):\n",
    "            print('>>> Print saved GP for PC # {}'.format(i))\n",
    "            print(f'GPR score is {GPR.score(design, pc_tf_data[:,i])} ')\n",
    "            print('Trained RBF params ', GPR.kernel_.get_params()['k1'])\n",
    "            print('Trained White Kernel params', GPR.kernel_.get_params()['k2'])\n",
    "            print(f'GPR log_marginal likelihood {GPR.log_marginal_likelihood()} \\n')\n",
    "else:\n",
    "    Emulators = []\n",
    "    for i in range(0,15):\n",
    "        start_time = time.time()\n",
    "        print('>>> Training GP for PC # {}'.format(i))\n",
    "        kernel = 1 * krnl.RBF(length_scale=ptp,length_scale_bounds=np.outer(ptp, (.1, 1e2)))\\\n",
    "               + krnl.WhiteKernel(noise_level=.1, noise_level_bounds=(1e-4, 1e4))\n",
    "        GPR=gpr(kernel=kernel,n_restarts_optimizer=5)\n",
    "        GPR.fit(design, pc_tf_data[:,i].reshape(-1,1))\n",
    "        print(f'GPR score is {GPR.score(design, pc_tf_data[:,i])} ')\n",
    "        print('Trained RBF params ', GPR.kernel_.get_params()['k1'])\n",
    "        print('Trained White Kernel params', GPR.kernel_.get_params()['k2'])\n",
    "        print(f'GPR log_marginal likelihood {GPR.log_marginal_likelihood()} ')\n",
    "        print(\"--- %s seconds --- \\n\" % (time.time() - start_time))\n",
    "        Emulators.append(GPR)\n",
    "\n",
    "if (overide==True) or not(os.path.exists(data_path(EMU))):\n",
    "    with open(data_path(EMU),\"wb\") as f:\n",
    "        pickle.dump(Emulators,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntruncated = 12\n",
    "#Scale Transformation from PC space to original data space\n",
    "inverse_tf_matrix= np.diag(s[:Ntruncated]) @ vh[:Ntruncated,:] * SS.scale_.reshape(1,110)/ math.sqrt(u.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_observables(model_parameters):\n",
    "    \"\"\"Predicts the observables for any model parameter value using the trained emulators.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Theta_input : Model parameter values. Should be an 1D array of 17 model parametrs.\n",
    "    Return\n",
    "    ------\n",
    "    Mean value and full error covaraiance matrix of the prediction is returened. \"\"\"\n",
    "    \n",
    "    mean=[]\n",
    "    variance=[]\n",
    "    theta=np.array(model_parameters).flatten()\n",
    "    \n",
    "    if len(theta)!=17:\n",
    "        raise TypeError('The input model_parameters array does not have the right dimensions')\n",
    "    else: \n",
    "        for i in range(0,Ntruncated):\n",
    "            mn, std = Emulators[i].predict([theta], return_std=True)\n",
    "            mean.append(mn[0])\n",
    "            variance.append(std[0]**2)\n",
    "    mean = np.array(mean)\n",
    "    inverse_transformed_mean = mean.T @ inverse_tf_matrix + SS.mean_\n",
    "    variance_matrix = np.diag(np.array(variance))\n",
    "    inverse_transformed_variance=np.einsum('ik,kl,lj-> ij', inverse_tf_matrix.T, variance_matrix, inverse_tf_matrix, optimize=False)\n",
    "    return inverse_transformed_mean[0], inverse_transformed_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework Exercise\n",
    "To keep this notebook short we have omitted some of the essential checks in any Bayesian Parameter Inference task. Can you try to build these steps on your own?\n",
    " - Emulator validation\n",
    " - Prior predictive checks\n",
    " - Closure test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bayesian parameter inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_validation = False\n",
    "validation_set = 0\n",
    "Yexp = y_exp if not do_validation else validation.iloc[0].values[1:]\n",
    "Yvar = y_exp_variance if not do_validation else Yexp**2/100.**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an uniform prior in the 17 parameters as is used in the two JETSCAPE publications.\n",
    "# Howework: what happens if you use an informative prior?\n",
    "# or an more restrictive prior\n",
    "# for example: What if you are (somehow) extremely certain that the nucleon width should be around 0.5 fm.\n",
    "def log_prior(model_parameters):\n",
    "    \"\"\"Evaluvate the prior at model prameter values. \n",
    "    If all parameters are inside bounds function will return 0 otherwise -inf\"\"\"\n",
    "    X = np.array(model_parameters).reshape(1,-1)\n",
    "    lower = np.all(X >= design_min)\n",
    "    upper = np.all(X <= design_max)\n",
    "    if (lower and upper):\n",
    "        lp=0\n",
    "    # lp = np.log(st.beta.pdf(X,5,1,dsgn_min_ut.reshape(1,-1),(dsgn_max_ut-dsgn_min_ut).reshape(1,-1))).sum()\n",
    "    else:\n",
    "        lp = -np.inf\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvn_loglike(y, cov):\n",
    "    \"\"\"\n",
    "    Evaluate the multivariate-normal log-likelihood for difference vector `y`\n",
    "    and covariance matrix `cov`:\n",
    "\n",
    "        log_p = -1/2*[(y^T).(C^-1).y + log(det(C))] + const.\n",
    "\n",
    "    The likelihood is NOT NORMALIZED, since this does not affect MCMC.  The\n",
    "    normalization const = -n/2*log(2*pi), where n is the dimensionality.\n",
    "\n",
    "    Arguments `y` and `cov` MUST be np.arrays with dtype == float64 and shapes\n",
    "    (n) and (n, n), respectively.  These requirements are NOT CHECKED.\n",
    "\n",
    "    The calculation follows algorithm 2.1 in Rasmussen and Williams (Gaussian\n",
    "    Processes for Machine Learning).\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the Cholesky decomposition of the covariance.\n",
    "    # Use bare LAPACK function to avoid scipy.linalg wrapper overhead.\n",
    "    L, info = lapack.dpotrf(cov, clean=False)\n",
    "\n",
    "    if info < 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "    elif info < 0:\n",
    "        raise np.linalg.LinAlgError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the leading minor of order {} is not positive definite'\n",
    "            .format(info)\n",
    "        )\n",
    "\n",
    "    # Solve for alpha = cov^-1.y using the Cholesky decomp.\n",
    "    alpha, info = lapack.dpotrs(L, y)\n",
    "\n",
    "    if info != 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrs error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "    #print(L.diagonal())\n",
    "    #a=np.ones(len(L.diagonal()))*1e-10\n",
    "    #print(a)\n",
    "    #print(L)\n",
    "    #L=L+np.diag(a)\n",
    "    if np.all(L.diagonal()>0):\n",
    "        return -.5*np.dot(y, alpha) - np.log(L.diagonal()).sum()\n",
    "    else:\n",
    "        print(L.diagonal())\n",
    "        raise ValueError(\n",
    "            'L has negative values on diagonal {}'.format(L.diagonal())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance truncation error from PC is not yet included\n",
    "exp_var = np.diag(Yvar)\n",
    "\n",
    "def log_like(model_parameters):\n",
    "    mn, var = predict_observables(model_parameters)\n",
    "    dy = mn - Yexp\n",
    "    return mvn_loglike(dy, var + exp_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC\n",
    "\n",
    "Here we used the Parallel Tempering MCMC Algorithm ([Will Vousden, Will M. Farr, Ilya Mandel (2015)](https://arxiv.org/pdf/1501.05823.pdf)) as implemented in the (PT)EMCEE package ([Foreman-Mackey, Hogg, Lang & Goodman (2012)](https://arxiv.org/abs/1202.3665)).\n",
    "\n",
    "The Parallel Tempering MCMC method creates a ladder of chains with different \"temperature\" (T) sampled according to \n",
    "\n",
    "$$ {\\rm Posterior_i}(p|{\\rm Exp}) = {\\rm Likelihood}(p|{\\rm Exp})^{1/T_i} \\times {\\rm Prior}(p)$$\n",
    "\n",
    "The temperatures for each chain: \n",
    "$$1=T_0< T_1 < T_2 < \\cdots T_n$$\n",
    "The cold chain with $T_0=1$ is the target posterior function that we need. High-temperature chains flatten the strucutres of the (possibily high-correlated) likelihood function to fast explore the entire parameter space. Different chains exchange configurations with probability $\\min\\left\\{1,\\left(\\frac{L_i}{L_j}\\right)^{1/T_j-1/T_i}\\right\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we actually perform the MCMC Sampling\n",
    "fname = 'ptemcee-exp' if not do_validation else 'ptemcee-validation-{:d}'.format(validation_set)\n",
    "filename = data_path(name + fname)\n",
    "# 1. Define the parameters\n",
    "run_mcmc = False\n",
    "ntemps = 10\n",
    "Tmax = 10\n",
    "\n",
    "ndim = 17\n",
    "nwalkers = ndim*10 # Typically 10*ndim\n",
    "nburnin = 200 # The number of steps it takes for the walkers to thermalize\n",
    "niterations= 1000 # The number of samples to draw once thermalized\n",
    "nthin = 4 # Record every nthin-th iteration\n",
    "\n",
    "nthreads = 1 # Easy parallelization! \n",
    "\n",
    "#min_theta = [1.625] # Lower bound for initializing walkers\n",
    "#max_theta = [24.79] # Upper bound for initializing walkers\n",
    "\n",
    "st = time.time()\n",
    "if run_mcmc:\n",
    "\n",
    "    # 2. Instantiate the sampler object with the parameters, data, likelihood, and prior.\n",
    "    \n",
    "    #sampler=PTSampler(ntemps, nwalkers, ndim, logl, logp, threads=nthreads, betas=betas)\n",
    "    ptsampler_ex = ptemcee.Sampler(nwalkers, ndim, log_like, log_prior, ntemps, \n",
    "                                   threads=nthreads, Tmax=Tmax)\n",
    "\n",
    "    # 3. Initialize the walkers at random positions in our 99% prior range\n",
    "    pos0 = design_min + (design_max - design_min) * np.random.rand(ntemps, nwalkers, ndim)\n",
    "\n",
    "    # 4. Run the sampler's burn-in iterations\n",
    "    print(\"Running burn-in phase\")\n",
    "    for p, lnprob, lnlike in ptsampler_ex.sample(pos0, iterations=nburnin,adapt=True):\n",
    "        pass\n",
    "    ptsampler_ex.reset() # Discard previous samples from the chain, but keep the position\n",
    "    et = time.time()\n",
    "    print(f'Time for burn-in {et-st}')\n",
    "    \n",
    "    \n",
    "    print(\"Running MCMC chains\")\n",
    "    # 5. Now we sample for nwalkers*niterations, recording every nthin-th sample\n",
    "    for p, lnprob, lnlike in ptsampler_ex.sample(p, iterations=niterations, thin=nthin,adapt=True):\n",
    "        pass \n",
    "\n",
    "    print('Done MCMC')\n",
    "\n",
    "    mean_acc_frac = np.mean(ptsampler_ex.acceptance_fraction)\n",
    "    print(f\"Mean acceptance fraction: {mean_acc_frac:.3f}\",\n",
    "          f\"(in total {nwalkers*niterations} steps)\")\n",
    "    \n",
    "    \n",
    "    et = time.time()\n",
    "    print(f'Time it took to generate MCMC chain {et-st}')\n",
    "    \n",
    "    # We only analyze the zero temperature MCMC samples\n",
    "    #np.save(name+'ptemcee', )\n",
    "    # closure_ex_chain = ptsampler_ex.chain[0, :, :, :].reshape((-1, ndim))\n",
    "    samples = ptsampler_ex.chain[0, :, :, :].reshape((-1, ndim))\n",
    "    samples_df= pd.DataFrame(samples, columns=model_param_dsgn)\n",
    "    samples_df.to_csv(filename)\n",
    "else:\n",
    "    print(\"Load pregenerated chain\")\n",
    "    samples_df = pd.read_csv(filename, index_col=0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples_df.shape)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework exercise\n",
    " - Can you perform some MCMC diagnostic tests that you know on these posterior chains?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_index_to_plot = [0, 1, 2 ,3 , 4, 5, 6, 15, 16]\n",
    "nplots = len(params_index_to_plot)\n",
    "fig, axes = plt.subplots(nplots,nplots,figsize=(16,16))\n",
    "\n",
    "Truth = design_validation.iloc[validation_set][1:]\n",
    "\n",
    "Nbins=15\n",
    "# Diagonal: marginalized single-parameter posterior\n",
    "for n,i in enumerate(params_index_to_plot):\n",
    "    ax = axes[n][n]\n",
    "    \n",
    "    H, _, _ = ax.hist(samples_df.iloc[:,i], histtype='step', color='r', density=True, bins=Nbins, lw=2)\n",
    "    x05, x50, x95 = np.quantile(samples_df.iloc[:,i], [.05, .5, .95])\n",
    "    ax.set_title(r'${:.3f}^{{-{:.3f} }}_{{+{:.3f} }}$'.format(x50, x50-x05, x95-x50) )\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "    if do_validation:\n",
    "        ax.plot([Truth[i], Truth[i]], [0,H.max()], 'k-', lw=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(design_min[i], design_max[i] )\n",
    "    axes[-1][n].set_xlabel(samples_df.columns[i])\n",
    "    axes[n][0].set_ylabel(samples_df.columns[i])\n",
    "    \n",
    "\n",
    "# Off diagonal: marginalized two-parameter posterior\n",
    "for n, i in enumerate(params_index_to_plot):\n",
    "    ylim = design_min[i], design_max[i] \n",
    "    for m, j in enumerate(params_index_to_plot):\n",
    "        ax = axes[n][m]\n",
    "        xlim = design_min[j], design_max[j] \n",
    "        if n<m:\n",
    "            ax.axis('off')\n",
    "        if n>m:\n",
    "            H, bx, by = np.histogram2d(samples_df.iloc[:,j], samples_df.iloc[:,i], range=(xlim,ylim), bins=Nbins)\n",
    "            ax.contourf((bx[1:]+bx[:-1])/2., (by[1:]+by[:-1])/2., H.T, levels=10, cmap=plt.cm.coolwarm)\n",
    "            ax.set_xlim(*xlim)\n",
    "            ax.set_ylim(*ylim)\n",
    "            if not ax.is_first_col():\n",
    "                ax.set_yticks([])    \n",
    "            if not ax.is_last_row():\n",
    "                ax.set_xticks([])\n",
    "            ax.tick_params(axis='both', labelsize=10)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "save_fig(name+\"partial\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_index_to_plot = [7,8,9,10,\n",
    "                       11,12,13,14]\n",
    "nplots = len(params_index_to_plot)\n",
    "fig, axes = plt.subplots(nplots,nplots,figsize=(12,12))\n",
    "\n",
    "Nbins=15\n",
    "# Diagonal: marginalized single-parameter posterior\n",
    "for n,i in enumerate(params_index_to_plot):\n",
    "    ax = axes[n][n]\n",
    "    H, _, _ = ax.hist(samples_df.iloc[:,i], histtype='step', color='r', density=True, bins=Nbins, lw=2)\n",
    "    x05, x50, x95 = np.quantile(samples_df.iloc[:,i], [.05, .5, .95])\n",
    "    ax.set_title(r'${:.3f}^{{-{:.3f} }}_{{+{:.3f} }}$'.format(x50, x50-x05, x95-x50) )\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "    if do_validation:\n",
    "        ax.plot([Truth[i], Truth[i]], [0,H.max()], 'k-', lw=1)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(design_min[i], design_max[i] )\n",
    "    axes[-1][n].set_xlabel(samples_df.columns[i])\n",
    "    axes[n][0].set_ylabel(samples_df.columns[i])\n",
    "    \n",
    "    \n",
    "# Off diagonal: marginalized two-parameter posterior\n",
    "for n, i in enumerate(params_index_to_plot):\n",
    "    ylim = design_min[i], design_max[i] \n",
    "    for m, j in enumerate(params_index_to_plot):\n",
    "        ax = axes[n][m]\n",
    "        xlim = design_min[j], design_max[j] \n",
    "        if n<m:\n",
    "            ax.axis('off')\n",
    "        if n>m:\n",
    "            H, bx, by = np.histogram2d(samples_df.iloc[:,j], samples_df.iloc[:,i], range=(xlim,ylim), bins=Nbins)\n",
    "            ax.contourf((bx[1:]+bx[:-1])/2., (by[1:]+by[:-1])/2., H.T, levels=10, cmap=plt.cm.coolwarm, vmin=0)\n",
    "            ax.set_xlim(*xlim)\n",
    "            ax.set_ylim(*ylim)\n",
    "            if not ax.is_first_col():\n",
    "                ax.set_yticks([])    \n",
    "            if not ax.is_last_row():\n",
    "                ax.set_xticks([])\n",
    "            ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "save_fig(name+\"viscos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP (parameter set that maximize the posterior) predictions compared to experimental observables\n",
    "\n",
    "*MAP is just a representative parameter set in the high-likeligood region of posterior. Its exact loaction can certainty change depending on your prior and parametrization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this is taking so long, please use the following MAP for the comparison to exp data with 12 PC\n",
    "use_saved_MAP = True\n",
    "\n",
    "if not use_saved_MAP:\n",
    "    bounds=[(a,b) for (a,b) in zip(design_min,design_max)]\n",
    "    \"\"\"rslt = optimize.differential_evolution(lambda x: -log_like(x)-log_prior(x), \n",
    "                                           bounds=bounds,\n",
    "                                           maxiter=10000,\n",
    "                                          disp=True,\n",
    "                                          tol=1e-9,\n",
    "                                         )\"\"\"\n",
    "\n",
    "    # We will starts the optimziation with different initial locations to avoid local minima\n",
    "    R = np.random.rand(1000, *design_min.shape)\n",
    "    Xs = design_min*(1-R) + design_max*R\n",
    "    Y = [log_like(x)+log_prior(x) for x in Xs]\n",
    "    start_x = Xs[np.argmax(Y)]\n",
    "\n",
    "\n",
    "    R = np.random.rand(*design_min.shape)\n",
    "    x0 = design_min*(1-R) + design_max*R\n",
    "    res = optimize.minimize(lambda x: -log_like(x)-log_prior(x),  x0=start_x, bounds=bounds, tol=1e-8)\n",
    "    Map_param = res.x\n",
    "    print(res.fun)\n",
    "else:\n",
    "    Map_param = np.array([14.26370278, -0.0347678,   0.9610322,   0.96907836,  0.92296974,  1.41774572,\n",
    "                        0.08480153,  0.25015421, -0.15067507, -0.18580838,  0.13378421,  0.12772325,\n",
    "                      0.1790854,   0.09202676,  0.16874745,  4.75186113,  0.13424626])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Map_param)\n",
    "MAP_mean, MAP_var = predict_observables(Map_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 5), sharex=True)\n",
    "axes =  axes.flatten()\n",
    "for row, obs_group in enumerate(obs_groups.keys()):\n",
    "    for obs, color in zip(obs_groups[obs_group], colors):\n",
    "        expt_label = 'ALICE'\n",
    "        \n",
    "        axes[row].tick_params(labelsize=9)\n",
    "\n",
    "        scale = 1.0\n",
    "        if obs_group == 'yields':\n",
    "            axes[row].set_yscale('log')\n",
    "            axes[row].set_title(\"MAP predictions\", fontsize = 11)          \n",
    "            if obs == 'dET_deta':\n",
    "                scale = 5.\n",
    "            if obs == 'dNch_deta':\n",
    "                scale = 2.\n",
    "        \n",
    "        axes[row].set_ylabel(obs_group_labels[obs_group], fontsize = 11)\n",
    "        \n",
    "        xbins = np.array(obs_cent_list['Pb-Pb-2760'][obs])\n",
    "        x = (xbins[:,0] + xbins[:,1]) / 2.\n",
    "\n",
    "        label = obs_tex_labels[obs]    \n",
    "        \n",
    "        # MAP:\n",
    "        Y1 = MAP_mean[index[obs][0]:index[obs][1]]\n",
    "        Yerr1 = np.sqrt(MAP_var.diagonal()[index[obs][0]:index[obs][1]])    \n",
    "        axes[row].plot(x, Y1*scale, '--', color = color, label = label, lw = 1.5)\n",
    "        axes[row].fill_between(x, (Y1-Yerr1)*scale, (Y1+Yerr1)*scale, color=color, alpha=0.2)\n",
    "        \n",
    "          \n",
    "        exp_mean = y_exp[index[obs][0]:index[obs][1]]\n",
    "        exp_err = np.sqrt(y_exp_variance[index[obs][0]:index[obs][1]])\n",
    "\n",
    "        axes[row].errorbar(x, exp_mean*scale, exp_err, color='black', fmt='v', markersize='4', elinewidth=1)\n",
    "        \n",
    "    leg = axes[row].legend(fontsize=9, borderpad=0, labelspacing=0, handlelength=1, handletextpad=0.2)\n",
    "    \n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(2.0)\n",
    "        legobj.set_alpha(1.0)\n",
    "\n",
    "    axes[row].set_xlim(0, 70)\n",
    "\n",
    "    if obs_group == 'yields':\n",
    "        axes[row].set_ylim(1, 1e4)\n",
    "    if obs_group == 'mean_pT':\n",
    "        axes[row].set_ylim(0., 1.5)\n",
    "    if obs_group == 'fluct':\n",
    "        axes[row].set_ylim(0.0, 0.04)\n",
    "    if obs_group == 'flows':\n",
    "        axes[row].set_ylim(0.0, 0.12)\n",
    "    if axes[row].is_last_row():\n",
    "        axes[row].set_xlabel('Centrality %', fontsize = 11)\n",
    "\n",
    "plt.tight_layout(True)\n",
    "set_tight(fig, rect=[0, 0, 1, 1])\n",
    "save_fig(\"MAP_prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior for the viscosities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.linspace(0.12, .5, 50)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,4),\n",
    "                         sharex=False, sharey=False, constrained_layout=True)\n",
    "\n",
    "fig.suptitle(\"Specefic shear viscosity posterior\", wrap=True)\n",
    "\n",
    "\n",
    "if do_validation:\n",
    "    true_etas = eta_over_s(T, *Truth[7:11])\n",
    "    ax.plot(T, true_etas, 'k--', label='Truth', lw=2)\n",
    "print(Map_param[7:11])\n",
    "posterior_etas = []\n",
    "for row in samples_df.iloc[:,7:11].values:\n",
    "    T_k, alow, ahigh, etas_k = row\n",
    "    posterior_etas.append(eta_over_s(T, T_k ,alow, ahigh, etas_k))\n",
    "posterior_etas = np.array(posterior_etas)\n",
    "\n",
    "Median = np.percentile(posterior_etas, 50, axis=0)\n",
    "ax.plot(T, Median, 'r-', label=\"Marginalzied median\", lw=2)\n",
    "ax.plot(T, eta_over_s(T, *Map_param[7:11]), 'b--', label='MAP', lw=2)\n",
    "for CL in [60,90,95]:\n",
    "    Low, High = np.percentile(posterior_etas, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(T, Low, High, alpha=0.2, color='r')\n",
    "    ax.annotate(\"{}%CL\".format(CL), xy=(T[-1]*1.02, High.max()), va=\"center\", ha=\"left\")\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('T [GeV]')\n",
    "ax.set_ylabel('$\\eta/s$')\n",
    "save_fig('shear_pos_ptemcee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.linspace(0.12, .5, 50)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,4),\n",
    "                         sharex=False, sharey=False, constrained_layout=True)\n",
    "\n",
    "fig.suptitle(\"Specefic bulk viscosity posterior\", wrap=True)\n",
    "\n",
    "if do_validation:\n",
    "    true_zetas = zeta_over_s(T, *Truth[11:15])\n",
    "    ax.plot(T, true_zetas, 'k--', label='Truth', lw=2)\n",
    "\n",
    "posterior_zetas = []\n",
    "for row in samples_df.iloc[:,11:15].values:\n",
    "    T_k, alow, ahigh, etas_k = row\n",
    "    posterior_zetas.append(zeta_over_s(T, T_k ,alow, ahigh, etas_k))\n",
    "posterior_zetas = np.array(posterior_zetas)\n",
    "\n",
    "Median = np.percentile(posterior_zetas, 50, axis=0)\n",
    "ax.plot(T, Median, 'r-', label=\"Marginalzied median\", lw=2)\n",
    "ax.plot(T, zeta_over_s(T, *Map_param[11:15]), 'b--', label='MAP', lw=2)\n",
    "for CL in [60,90,95]:\n",
    "    Low, High = np.percentile(posterior_zetas, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(T, Low, High, alpha=0.2, color='r')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('T [GeV]')\n",
    "ax.set_ylabel('$\\zeta/s$')\n",
    "save_fig('bulk_pos_ptemcee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: over lay the posterior with the prior. Do all the features in the posterior come from experimental data constraints?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the information gain using the Kullback–Leibler (K-L) divergence\n",
    "\n",
    "You have seen that at certain temperatures, experimental data can reduce the prior band to a narrower posterior band; while in other regions (particularly the high-temperature region), the data do not provide much constraints. We can quantify this observation using the so-called Kullback–Leibler (K-L) divergence. \n",
    "\n",
    "Consider the prior and the posterior distribution of $\\zeta/s$ at a certain temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.linspace(0.12, .5, 50)\n",
    "\n",
    "u4 = np.random.rand(4, 500000)\n",
    "Tk    =  0.13 * (1.-u4[0]) + 0.3 * u4[0]\n",
    "alow  = - 2.0 * (1.-u4[1]) + 1.0 * u4[1] \n",
    "ahigh = - 1.0 * (1.-u4[2]) + 2.0 * u4[2] \n",
    "etask =  0.01 * (1.-u4[3]) + 0.2 * u4[3] \n",
    "prior_etas = np.array([eta_over_s(it, Tk, alow, ahigh, etask) for it in T]).T\n",
    "\n",
    "zetasmax =  0.01 * (1.-u4[0]) + 0.2 * u4[0]\n",
    "Tzeta  = 0.12 * (1.-u4[1]) + 0.3 * u4[1] \n",
    "wzeta = 0.025 * (1.-u4[2]) + 0.15 * u4[2] \n",
    "lambdazeta =  -0.8 * (1.-u4[3]) + 0.8 * u4[3] \n",
    "prior_zetas = np.array([zeta_over_s(it, zetasmax, Tzeta, wzeta, lambdazeta) for it in T]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iT = 10\n",
    "print(\"T = {:1.3f} GeV\".format(T[iT]))\n",
    "plt.figure(figsize=(3.5,3))\n",
    "_ = plt.hist(prior_etas[:,iT], density=True, range=[0,prior_etas[:,iT].max()], bins=51, \n",
    "             histtype='step', color='b', lw=2, label=\"Prior\")\n",
    "_ = plt.hist(posterior_etas[:,iT], density=True, range=[0,prior_etas[:,iT].max()], bins=51, \n",
    "             histtype='step', color='r', lw=2, label=\"Posterior\")\n",
    "_ = plt.xlabel(r\"$\\eta/s$\")\n",
    "_ = plt.ylabel(r\"$P(\\eta/s)$ at $T={:1.3f}$ GeV\".format(T[iT]))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain $\\Leftrightarrow$ How different are the two distributions (Prior v.s. Posterior).\n",
    "\n",
    "The K-L divergence is a measure of the diference/distance of two distributions $P(x), Q(x)$,\n",
    "\n",
    "$$D_{KL}(P||Q) = \\int P(x)\\ln \\frac{P(x)}{Q(x)} dx > 0$$\n",
    "\n",
    "Let's take $Q$ to be the posterior and $P$ to be the prior. We will use the histrogramed values to approxiamte the above integral\n",
    "\n",
    "For example if P and Q are two Gaussian with parameters $\\mu_P, \\sigma_P$ and $\\mu_Q, \\sigma_Q$, then\n",
    "$$D_{KL}(P||Q) = \\frac{(\\mu_P-\\mu_Q)^2}{\\sigma_Q^2} +  \\frac{\\sigma_P^2}{\\sigma_Q^2}\\left(1- \\frac{\\sigma_Q^2}{\\sigma_P^2}\\ln\\frac{\\sigma_P^2}{\\sigma_Q^2}\\right)-1 $$\n",
    "It is easy to see that given a prior $Q$, the K-L divergence is only zero when the two parameters of P and Q matches (i.e., no extra information is gained in the posterior).\n",
    "K-L divergence is large if\n",
    "   - $\\mu_P$ to be very different from $\\mu_Q$, i.e., learned something less expected.\n",
    "   - $\\mu_P \\ll \\mu_Q$, i.e., learning something with more certainty.\n",
    "   - $\\mu_P > \\mu_Q$, not allowed in our set-up...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def KL(iT, Y='etas', Nb=101):\n",
    "    S0 = prior_etas[:,iT] if Y=='etas' else prior_zetas[:,iT]\n",
    "    S1 = posterior_etas[:,iT]if Y=='etas' else posterior_zetas[:,iT]\n",
    "    Max = S0.max()\n",
    "    Hprior, bx = np.histogram(S0, density=True, bins=Nb, range=(0, Max))\n",
    "    Hposterior, bx = np.histogram(S1, density=True, bins=Nb, range=(0, Max))\n",
    "    dx = bx[1] - bx[0]\n",
    "    return ( Hposterior * np.log(1e-20 + Hposterior/Hprior ) )[(Hposterior>0) & (Hprior >0)].sum() * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KL divergence at T=\", T[iT], \"GeV is\", KL(iT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(T, [KL(iT, 'etas') for iT, _ in enumerate(T)], 'k-')\n",
    "plt.xlim(0.12, 0.5)\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlabel(r\"$T$ [GeV]\")\n",
    "plt.ylabel(r\"$D_{KL}(P_1||P_0)$\")\n",
    "plt.title(\"Information gain of $\\eta/s$\")\n",
    "save_fig(\"KL-etas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.plot(T, [KL(iT, 'zetas') for iT, _ in enumerate(T)], 'k-')\n",
    "plt.xlim(0.12, 0.5)\n",
    "plt.ylim(ymin=0)\n",
    "plt.xlabel(r\"$T$ [GeV]\")\n",
    "plt.ylabel(r\"$D_{KL}(P_1||P_0)$\")\n",
    "plt.title(\"Information gain of $\\zeta/s$\")\n",
    "save_fig(\"KL-zetas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is not covered\n",
    "\n",
    "> Emulator validation\n",
    "\n",
    "> A full closure tests (all the 100 validation points to quantify the performance in the entire parameter space)\n",
    "\n",
    "> MCMC convergence\n",
    "\n",
    "> Predictive power of the calibrated model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
